{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Heatmaps from any Sequencing Experiment\n",
    "\n",
    "This notebook contains python code which I use to construct heatmaps comparing samples in different Sequencing Experiments. The notebook `R Code` contains the code used to construct heatmaps using the R package Pheatmap and edgeR for normalization.\n",
    "\n",
    "**Note** All this code can also be written in R. However, I prefer using Python as much as I can, which is why I wrote this in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a list of Consensus Sequences\n",
    "\n",
    "With RNA-seq Heatmaps, each row represents a gene, making alignment of RNA-seq data to a heatmap relatively straightforward. However, with other sequencing experiments such as ATAC-seq, there are no such strict boundaries on what constitutes the rows of the heatmap. I have personally experimented with a variety of different possible ways to delimit the row in a heatmap, including tiling across the genome, and using each tile as a row, and creating a general consensus sequence of overlap between sample sequences. And I have found that using a consensus sequence provides the best results.\n",
    "\n",
    "While there are no doubt many ways to go about creating this list, the most straightforward method that I have found is to use bedtools and a custom python script to find all regions of overlap, and save the read counts as a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get some data to play with!\n",
    "\n",
    "I am going to download [liver](https://www.encodeproject.org/experiments/ENCSR785NEL/) and [brain](https://www.encodeproject.org/experiments/ENCSR273UFV/) BAM files from ENCODE to use as tutorial data. BAM peaks will then be called so I can get the genomic loci for each file and the read counts at each loci. There are a variety of different programs which call peaks, so use whichever method works best for you. Here are some other methods which you can use:\n",
    "\n",
    "* [Genrich](https://github.com/jsh58/Genrich)\n",
    "* [MACS2](https://github.com/taoliu/MACS)\n",
    "* [HOMER](http://homer.ucsd.edu/homer/ngs/peaks.html)\n",
    "* [HMMRATAC](https://github.com/LiuLabUB/HMMRATAC)\n",
    "\n",
    "Genrich and MACS2 are the current standards, though Genrich has not been published yet. I will be using Genrich in this tutorial, but Galaxy also has a great [tutorial](https://galaxyproject.github.io/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html#peak-calling) on MACS2 if you want to use that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess, os\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_genrich(bam,outpath,path=0):\n",
    "    # given some bam, run genrich and send narrowPeak and bed output to output\n",
    "    # path to genrich script can be specified. Otherwise, will assume that `Genrich` is the path\n",
    "    if '/' in bam:\n",
    "        header = bam.split('/')[-1].split('.bam')[0]\n",
    "    elif '.bam' in bam:\n",
    "        header = bam.split('.bam')[0]\n",
    "    else:\n",
    "        print(\"ERROR, please use a valid BAM file\")\n",
    "    if outpath[-1] != '/':\n",
    "        outpath += '/'\n",
    "        \n",
    "    outnarrow = outpath + header + '.narrowPeak'\n",
    "    outbed = outpath + header + '.bed'\n",
    "    if path == 0:\n",
    "        path = 'Genrich'\n",
    "        \n",
    "    subprocess.run(path+ ' -t ' + bam + ' -o ' + outnarrow + \" -b \" + outbed + \" -r -v\",shell=True,check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamdir = '/mnt/labshare/chromatin-datasets/'\n",
    "filenames = ['liver-p1.sorted.bam', 'liver-p2.sorted.bam','brain-p1.sorted.bam','brain-p2.sorted.bam']\n",
    "\n",
    "# I have my bam files stored in the directory `/mnt/labshare/chromatin-datasets/`\n",
    "# For the specified filenames in said directory, run genrich\n",
    "\n",
    "for i in os.listdir(bamdir):\n",
    "    if i in filenames:\n",
    "        run_genrich(bamdir+i,\".\",'Genrich')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have bed files of our peaks! \n",
    "\n",
    "Next, we need to get the read counts of these loci. We can do this using `bedtools coverage`. \n",
    "\n",
    "example:\n",
    "\n",
    "`bedtools coverage -a liver-p1.bed -b liver-p1.sorted.bam > liver-p1.read.bed`\n",
    "\n",
    "I recommend using pybedtools or subprocess to automate the creation of these read files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our read files, we can work on decreasing the size of our giant bed files. The best way to do this is to use `bedtools merge`.\n",
    "\n",
    "example:\n",
    "\n",
    "`bedtools merge -i liver-p1.read.bed -c 5 -o sum > liver-p1.mergedread.bed`\n",
    "\n",
    "By default there will be 5 columns in each bed file, with the fifth having the read counts. Merging BED files will compress all overlaps in a file. In the above command, we instruct bedtools to sum the contents of column 5 (the readcounts) for all overlaps that are merged. Note this method is not perfect, and that because we are using merging here, the distribution of reads will appear to be even across regions, even if there is actually higher readcount density in certain regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our merged files. Lets combine them all, sort them, then merge them so that we get our consensus sequences. \n",
    "\n",
    "`cat *.mergedread.bed | sort -k 1,1 -k2,2n | bedtools merge > consensus.bed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What files do we have now?\n",
    "\n",
    "Ok, that was a lot. Lets review what files we have created so far.\n",
    "\n",
    "1. We have bam files for every sample and experiment.\n",
    "2. We have bed files and narrowPeak files for the peak calls in those experiments.\n",
    "3. We have bed files with read counts per sample per experiment.\n",
    "4. We have merged bed files per sample per experiment.\n",
    "5. We have a consensus bed sequence.\n",
    "\n",
    "For the creation of our heatmap, we only need the merged bed files per sample per experiment and the consensus bed sequence (4 and 5) going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a read count matrix\n",
    "\n",
    "Now that we have our consensus bed sequences, we can use it to create a matrix of read counts per loci for each sample. You can run the below code to create your matrix. However, running such code in jupyter is significantly slower than running it in terminal as a script (4 hours versus 2.5 hours). So, I suggest using the `makematrix.py` script provided, which acts essentially the same as the below function.\n",
    "\n",
    "ex: `python makematrix.py data data/consensus.bed data/output-matrix`\n",
    "\n",
    "In the above, we take in a directory of bed files, a consensus sequence, and specify an output for the matrix.\n",
    "\n",
    "I recommend running this on a server. It can take anywhere from 30 minutes to 3 hours depending on the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beddir_to_matrix(bed_directory, consensus, bespec = 0):\n",
    "    # Take in some directory with bed files with readconts, align to consensus and sum overlaps in same samples. Output results as a matrix\n",
    "    # optionally can specify some filename header specificity to choose what bed files to read in\n",
    "    \n",
    "    # ex: beddir_to_matrix('data','data/consensus.bed')\n",
    "    \n",
    "    files = []\n",
    "    headers = []\n",
    "    if bed_directory[-1] != '/':\n",
    "        bed_directory += '/'\n",
    "    for i in os.listdir(bed_directory):\n",
    "        if type(bespec) == str:\n",
    "            if bespec in i:\n",
    "                files.append(bed_directory+i)\n",
    "                headers.append(i.split('.bed')[0])\n",
    "        else:\n",
    "            if '.bed' in i and consensus != i:\n",
    "                files.append(bed_directory+i)\n",
    "                headers.append(i.split('.bed')[0])\n",
    "    \n",
    "    # make dictionary\n",
    "    matrixrc = {}\n",
    "    matrixrc['loci'] = []\n",
    "    for i in headers:\n",
    "        matrixrc[i] = []\n",
    "    \n",
    "    # read files\n",
    "    consensus = pd.read_table(consensus,sep='\\t',header=None)\n",
    "    \n",
    "    # fill dictionary\n",
    "    total = len(consensus)\n",
    "    for index,row in consensus.iterrows():\n",
    "        chrom = row[0]\n",
    "        beg = str(row[1])\n",
    "        end = str(row[2])\n",
    "        matrixrc['loci'].append(chrom + \":\" + beg + \"-\" + end)\n",
    "        with open('temp','w') as out:\n",
    "            out.write(chrom+'\\t'+beg+'\\t'+end)\n",
    "        for n in range(len(files)):\n",
    "            s = files[n]\n",
    "            key = list(matrixrc.keys())[n+1]\n",
    "            subprocess.run(\"intersectBed -a \" + s + \" -b temp -wa > intertemp\",shell=True,check=True)\n",
    "            try:\n",
    "                inter = pd.read_table(\"intertemp\",header=None)\n",
    "                if len(inter) == 1:\n",
    "                    matrixrc[key].append(inter[3].to_list()[0])\n",
    "                elif len(inter) > 1:\n",
    "                    matrixrc[key].append(inter[3].sum())\n",
    "            except:\n",
    "                matrixrc[key].append(0)\n",
    "        clear_output(wait=True)\n",
    "        percent = (index+1)/total * 100\n",
    "        print(str(percent) + \"% complete\")\n",
    "        print(total)\n",
    "    os.remove(\"temp\")\n",
    "    os.remove(\"intertemp\")\n",
    "    pd.DataFrame.from_dict(matrixrc).to_csv('matrix',index=False)\n",
    "    return(pd.DataFrame.from_dict(matrixrc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3466666666666667% complete\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "beddir_to_matrix('.','xaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
